{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5889cc",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ea6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bestreads.text as text\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/goodreads_books.csv')\n",
    "data_test = data.sample(frac = 0.2, random_state = 111)\n",
    "data_train = data.drop(data_test.index)\n",
    "\n",
    "data_val = data_train.sample(frac=0.2, random_state=112)\n",
    "data_train = data_train.drop(data_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b66253",
   "metadata": {},
   "source": [
    "## Train-Test Set Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0212a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './data/processed/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "data_train.to_csv(save_dir + 'goodreads_books_train.csv', index = False)\n",
    "data_val.to_csv(save_dir + 'goodreads_books_val.csv', index=False)\n",
    "data_test.to_csv(save_dir + 'goodreads_books_test.csv', index = False)\n",
    "data_train.reset_index(inplace=True)\n",
    "data_val.reset_index(inplace=True)\n",
    "data_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe8710",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cc918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_english_description_train = text.is_english(data_train['description'])\n",
    "english_descriptions_train = data_train.description[is_english_description_train]\n",
    "cleaned_descriptions_train = text.clean_text(english_descriptions_train)\n",
    "is_english_description_train = is_english_description_train.rename('is_english_description',)\n",
    "cleaned_descriptions_train = cleaned_descriptions_train.rename('cleaned_descriptions')\n",
    "\n",
    "is_english_description_val = text.is_english(data_val['description'])\n",
    "english_descriptions_val = data_val.description[is_english_description_val]\n",
    "cleaned_descriptions_val = text.clean_text(english_descriptions_val)\n",
    "is_english_description_val = is_english_description_val.rename('is_english_description',)\n",
    "cleaned_descriptions_val = cleaned_descriptions_val.rename('cleaned_descriptions')\n",
    "\n",
    "is_english_description_test = text.is_english(data_test['description'])\n",
    "english_descriptions_test = data_test.description[is_english_description_test]\n",
    "cleaned_descriptions_test = text.clean_text(english_descriptions_test)\n",
    "is_english_description_test = is_english_description_test.rename('is_english_description')\n",
    "cleaned_descriptions_test = cleaned_descriptions_test.rename('cleaned_descriptions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12809e8",
   "metadata": {},
   "source": [
    "## Grouping Descriptions by Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680680e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/Documents/bestreads/bestreads/text.py:144: RuntimeWarning: NaN values detected in genre_and_votes; these will beskipped\n",
      "  + 'skipped', category=RuntimeWarning)\n",
      "100%|██████████████████████████████████| 28154/28154 [00:01<00:00, 18072.01it/s]\n",
      "100%|████████████████████████████████████| 7005/7005 [00:00<00:00, 18206.23it/s]\n",
      "100%|████████████████████████████████████| 8777/8777 [00:00<00:00, 18143.92it/s]\n"
     ]
    }
   ],
   "source": [
    "genre_and_votes_train = text.get_genres(data_train.genre_and_votes[is_english_description_train])\n",
    "genre_and_votes_val = text.get_genres(data_val.genre_and_votes[is_english_description_val])\n",
    "genre_and_votes_test = text.get_genres(data_test.genre_and_votes[is_english_description_test])\n",
    "combined = text.combine_genres(genre_and_votes_train.genre_1, cleaned_descriptions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa2ec3",
   "metadata": {},
   "source": [
    "## Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da387d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 52/52 [00:11<00:00,  4.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speculative Fiction</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Writing</th>\n",
       "      <th>Reference</th>\n",
       "      <th>War</th>\n",
       "      <th>Shapeshifters</th>\n",
       "      <th>Race</th>\n",
       "      <th>Plays</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Poetry</th>\n",
       "      <th>LGBT</th>\n",
       "      <th>Literature</th>\n",
       "      <th>Paranormal</th>\n",
       "      <th>Science Fiction</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Space</th>\n",
       "      <th>Literary Fiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>futur</th>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peopl</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep</th>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Speculative Fiction  Historical   Writing  Reference       War  \\\n",
       "thi               0.000000    0.000000  0.000000   0.000000  0.000000   \n",
       "futur             0.000277    0.000045  0.000029   0.000005  0.000038   \n",
       "peopl             0.000064    0.000040  0.000030   0.000020  0.000029   \n",
       "need              0.000059    0.000037  0.000044   0.000131  0.000075   \n",
       "sleep             0.000193    0.000035  0.000036   0.000123  0.000050   \n",
       "\n",
       "       Shapeshifters      Race     Plays    Horror   Holiday  ...  Politics  \\\n",
       "thi         0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "futur       0.000052  0.000017  0.000043  0.000014  0.000000  ...  0.000054   \n",
       "peopl       0.000018  0.000059  0.000022  0.000046  0.000000  ...  0.000040   \n",
       "need        0.000195  0.000012  0.000026  0.000085  0.000000  ...  0.000073   \n",
       "sleep       0.000053  0.000000  0.000021  0.000124  0.000222  ...  0.000000   \n",
       "\n",
       "         Poetry      LGBT  Literature  Paranormal  Science Fiction   Romance  \\\n",
       "thi    0.000000  0.000000    0.000000    0.000000         0.000000  0.000000   \n",
       "futur  0.000010  0.000028    0.000014    0.000006         0.000125  0.000063   \n",
       "peopl  0.000008  0.000023    0.000027    0.000012         0.000034  0.000022   \n",
       "need   0.000023  0.000131    0.000017    0.000079         0.000080  0.000156   \n",
       "sleep  0.000017  0.000000    0.000024    0.000054         0.000023  0.000040   \n",
       "\n",
       "        Fantasy     Space  Literary Fiction  \n",
       "thi    0.000000  0.000000          0.000000  \n",
       "futur  0.000065  0.000108          0.000022  \n",
       "peopl  0.000026  0.000028          0.000027  \n",
       "need   0.000097  0.000047          0.000040  \n",
       "sleep  0.000000  0.000085          0.000000  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_table_train = text.tf_idf(combined)\n",
    "\n",
    "# Here, we save the index because each index is a word\n",
    "tf_idf_table_train.to_csv(save_dir + 'tf_idf_table_train.csv', index_label='word')\n",
    "tf_idf_table_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41a0dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the training data\n",
    "processed_data_train = (is_english_description_train.to_frame()\n",
    "                        .merge(cleaned_descriptions_train,\n",
    "                               left_index=True, right_index=True,\n",
    "                               validate='1:1')\n",
    "                        .merge(genre_and_votes_train,\n",
    "                               left_index=True, right_index=True,\n",
    "                               validate='1:1'))\n",
    "\n",
    "# Here, we save the index because some rows are now missing due to\n",
    "# english language selection\n",
    "processed_data_train.to_csv(save_dir + 'goodreads_books_train_processed.csv', \n",
    "                            index_label='index')\n",
    "\n",
    "# Save the validation\n",
    "processed_data_val = (is_english_description_val.to_frame()\n",
    "                        .merge(cleaned_descriptions_val,\n",
    "                               left_index=True, right_index=True,\n",
    "                               validate='1:1')\n",
    "                        .merge(genre_and_votes_val,\n",
    "                               left_index=True, right_index=True,\n",
    "                               validate='1:1'))\n",
    "\n",
    "# Here, we save the index because some rows are now missing due to\n",
    "# english language selection\n",
    "processed_data_val.to_csv(save_dir + 'goodreads_books_val_processed.csv', \n",
    "                          index_label='index')\n",
    "\n",
    "# Save the test data\n",
    "processed_data_test = (is_english_description_test.to_frame()\n",
    "                        .merge(cleaned_descriptions_test,\n",
    "                               left_index=True, right_index=True,\n",
    "                               validate='1:1')\n",
    "                        .merge(genre_and_votes_test,\n",
    "                               left_index=True, right_index=True,\n",
    "                               validate='1:1'))\n",
    "\n",
    "# Here, we save the index because some rows are now missing due to\n",
    "# english language selection\n",
    "processed_data_test.to_csv(save_dir + 'goodreads_books_test_processed.csv', \n",
    "                           index_label='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
